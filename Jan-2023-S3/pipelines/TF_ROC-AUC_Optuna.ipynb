{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936b5758-0b17-4f59-bdfa-beb3ee47a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys, os\n",
    "#import seaborn as sns\n",
    "#from scipy import stats\n",
    "#from pathlib import Path\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.datasets import fetch_california_housing\n",
    "#import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "#import lightgbm as lgbm\n",
    "#from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "#from catboost import CatBoostRegressor\n",
    "#from lightgbm.sklearn import LGBMRegressor\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc,recall_score,precision_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import pickle\n",
    "import mplcyberpunk\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a1724a-47d5-4e16-abe6-cabcb3c82980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice, cycle\n",
    "\n",
    "plt.style.use(\"cyberpunk\")\n",
    "\n",
    "def add_secondary_plot(df, column, target_column, ax, n_bins, color=3, show_yticks=False, marker=\".\"):\n",
    "    secondary_ax = ax.twinx()\n",
    "    bins = pd.cut(df[column], bins=n_bins)\n",
    "    bins = pd.IntervalIndex(bins)\n",
    "    bins = (bins.left + bins.right) / 2\n",
    "    target = df.groupby(bins)[target_column].mean()\n",
    "    target.plot(\n",
    "        ax=secondary_ax, linestyle='',\n",
    "        marker=marker, color=color, label=f\"Mean '{target_column}'\"\n",
    "    )\n",
    "    secondary_ax.grid(visible=False)\n",
    "    \n",
    "    if not show_yticks:\n",
    "        secondary_ax.get_yaxis().set_ticks([])\n",
    "        \n",
    "    return secondary_ax\n",
    "\n",
    "def render_feature_distros(train_df, test_df, features=[], labels=[], n_bins=50, n_cols=4, pad=2, h_pad=4, w_pad=None):\n",
    "    histplot_hyperparams = {\n",
    "        'kde':True,\n",
    "        'alpha':0.4,\n",
    "        'stat':'percent',\n",
    "        'bins':n_bins\n",
    "    }\n",
    "    markers = ['.', '+', 'x', '1', '2']\n",
    "    \n",
    "    n_rows = math.ceil(len(features) / n_cols)\n",
    "    cell_with_dim = 4\n",
    "    cell_height_dim = 3\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols * cell_with_dim, n_rows * cell_height_dim))\n",
    "    plt.tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=None)\n",
    "    \n",
    "    # delete exess subplots\n",
    "    for a in ax[n_rows - 1, int(((n_rows - (len(features) / n_cols)) * n_cols*-1)):]:\n",
    "        a.axis('off')\n",
    "        \n",
    "    leg_handles = []\n",
    "    leg_labels = []\n",
    "    \n",
    "    axs = []\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        row = math.ceil(i / n_cols) - 1\n",
    "        col = (i % n_cols)\n",
    "        \n",
    "        color_cycle = islice(mplcyberpunk.cyberpunk_stylesheets['cyberpunk']['axes.prop_cycle'], 0, None)\n",
    "        \n",
    "        sns.histplot(train_df[feature], label='Train X', ax=ax[row, col], color=next(color_cycle)['color'], **histplot_hyperparams)\n",
    "        sns.histplot(test_df[feature], label='Test X', ax=ax[row, col],color=next(color_cycle)['color'], **histplot_hyperparams)\n",
    "        ax[row, col].set_title(f'{feature} Distribution')\n",
    "        mplcyberpunk.make_lines_glow(ax[row, col])\n",
    "        axs.append(ax[row, col].get_legend_handles_labels())\n",
    "\n",
    "        for j, label in enumerate(labels):\n",
    "            sub_ax = add_secondary_plot(train_df, feature, label, ax[row, col], n_bins, color=next(color_cycle)['color'], marker=markers[j])\n",
    "            axs.append(sub_ax.get_legend_handles_labels())\n",
    "        \n",
    "    for axis in axs:\n",
    "        if axis[1][0] not in leg_labels:\n",
    "            leg_labels.extend(axis[1])\n",
    "            leg_handles.extend(axis[0])\n",
    "        \n",
    "    fig.legend(leg_handles, leg_labels, loc='upper center', bbox_to_anchor=(0.5, 1.04), fontsize=14, ncol=len(features) + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf23cb5-cc6a-4dae-9dc1-7dc1b7b4021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = 1 * 60 * 60\n",
    "objective = 'tfregressor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30af7575-c694-4420-81e3-8f4608abc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.realpath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae92e2f-1d42-4d7f-953e-6cb3b5eb9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"..\\data\\train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(r\"..\\data\\test.csv\", index_col=0)\n",
    "sample_sub = pd.read_csv(r\"..\\data\\sample_submission.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433cce54-7033-4c6a-8fbb-bdee32110ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OverTime</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerformanceRating</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StandardHours</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusinessTravel</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Total  Percent\n",
       "Age                           0      0.0\n",
       "StockOptionLevel              0      0.0\n",
       "Over18                        0      0.0\n",
       "OverTime                      0      0.0\n",
       "PercentSalaryHike             0      0.0\n",
       "PerformanceRating             0      0.0\n",
       "RelationshipSatisfaction      0      0.0\n",
       "StandardHours                 0      0.0\n",
       "TotalWorkingYears             0      0.0\n",
       "BusinessTravel                0      0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ca2d44-448c-45a8-a500-15ffd3b3041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df['Over18']\n",
    "del train_df['EmployeeCount']\n",
    "del train_df['StandardHours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc257a3-0717-47f3-911a-40404ffbf02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BusinessTravel\n",
      "Department\n",
      "EducationField\n",
      "Gender\n",
      "JobRole\n",
      "MaritalStatus\n",
      "OverTime\n"
     ]
    }
   ],
   "source": [
    "columns_to_vectorize = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n",
    "for vector_target in columns_to_vectorize:\n",
    "    print(vector_target)\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit_transform(train_df[vector_target])\n",
    "    train_df[f'{vector_target}_v'] = vectorizer.transform(train_df[vector_target]).toarray().argmax(axis=1)[:,None]\n",
    "    vectorizer.fit_transform(test_df[vector_target])\n",
    "    test_df[f'{vector_target}_v'] = vectorizer.transform(test_df[vector_target]).toarray().argmax(axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d2b4c9f-993a-45f0-b216-294e6c732919",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction','HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', \n",
    "       'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n",
    "       'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n",
    "        'BusinessTravel_v', 'Department_v', 'EducationField_v','Gender_v', 'JobRole_v', 'MaritalStatus_v', 'OverTime_v']\n",
    "target = ['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed6f4daf-2da6-415b-9ef1-c9def5ae5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(train_df[features])\n",
    "X = scaler.transform(train_df[features])\n",
    "X_test = scaler.transform(test_df[features])\n",
    "\n",
    "scaler = MinMaxScaler().fit(train_df[target])\n",
    "Y = scaler.transform(train_df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0df4c6-ff19-49ab-9655-45f8fac57d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 12 \n",
    "FOLDS = 5\n",
    "param_grid_history = {}\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "\n",
    "def objective_v3(trial):\n",
    "    oof = np.zeros(len(X))\n",
    "    scores = []\n",
    "   \n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, Y)):\n",
    "        print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n",
    "        X_train, X_valid = train_df.iloc[train_idx][features], train_df.iloc[valid_idx][features]\n",
    "        y_train , y_valid = train_df[target].iloc[train_idx] , train_df[target].iloc[valid_idx] \n",
    "        \n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(train_df[features], train_df[target], test_size=0.20)\n",
    "\n",
    "        horsepower_model = tf.keras.Sequential([\n",
    "            layers.Dense(1000, activation=\"relu\"),\n",
    "            layers.Dropout(0.1),\n",
    "            layers.Dense(1000, activation=\"relu\"),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ])\n",
    "\n",
    "        #reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "        horsepower_model.compile(\n",
    "            loss='binary_crossentropy', optimizer='adam',metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        history = horsepower_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_valid, y_valid),\n",
    "\n",
    "            epochs=10,\n",
    "            # Suppress logging.\n",
    "            verbose=1,\n",
    "        )\n",
    "           \n",
    "        oof[valid_idx] = horsepower_model.predict(X_valid)[0]\n",
    "\n",
    "    roc_auc = roc_auc_score(Y, oof)\n",
    "    \n",
    "    param_grid_history[roc_auc] = {}\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02818b3a-e7d1-4624-809f-7d009b3aeb5e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-21 21:21:57,090]\u001b[0m A new study created in memory with name: xgbregressor\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Fold=1 ==========\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 1s 11ms/step - loss: 315.9835 - accuracy: 0.8113 - val_loss: 69.5992 - val_accuracy: 0.7024\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 35.5689 - accuracy: 0.8046 - val_loss: 82.1756 - val_accuracy: 0.8631\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 24.7279 - accuracy: 0.8143 - val_loss: 22.1791 - val_accuracy: 0.8423\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 28.1157 - accuracy: 0.8039 - val_loss: 18.4902 - val_accuracy: 0.8631\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 23.2843 - accuracy: 0.8098 - val_loss: 7.6418 - val_accuracy: 0.8661\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 13.3962 - accuracy: 0.8128 - val_loss: 14.5156 - val_accuracy: 0.8631\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 12.1440 - accuracy: 0.8106 - val_loss: 9.6673 - val_accuracy: 0.7857\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 13.3690 - accuracy: 0.8180 - val_loss: 12.7331 - val_accuracy: 0.8631\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 20.0516 - accuracy: 0.8106 - val_loss: 31.6420 - val_accuracy: 0.8631\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 14.9455 - accuracy: 0.8128 - val_loss: 11.8491 - val_accuracy: 0.8631\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "========== Fold=2 ==========\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 458.0118 - accuracy: 0.7949 - val_loss: 55.3711 - val_accuracy: 0.8929\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 31.4645 - accuracy: 0.7763 - val_loss: 8.9784 - val_accuracy: 0.8929\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 9.5478 - accuracy: 0.8285 - val_loss: 12.6865 - val_accuracy: 0.8929\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 13.2997 - accuracy: 0.7942 - val_loss: 2.0617 - val_accuracy: 0.6637\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 12.4588 - accuracy: 0.7964 - val_loss: 7.5855 - val_accuracy: 0.7530\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 7.1350 - accuracy: 0.8106 - val_loss: 1.5892 - val_accuracy: 0.8869\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 9.0732 - accuracy: 0.8031 - val_loss: 6.7713 - val_accuracy: 0.8929\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 7.8175 - accuracy: 0.8106 - val_loss: 1.6772 - val_accuracy: 0.8304\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 4.7218 - accuracy: 0.8016 - val_loss: 1.8883 - val_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 4.1582 - accuracy: 0.8084 - val_loss: 1.7510 - val_accuracy: 0.8869\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "========== Fold=3 ==========\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 280.9001 - accuracy: 0.7987 - val_loss: 20.6936 - val_accuracy: 0.8333\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 48.6920 - accuracy: 0.8046 - val_loss: 31.2557 - val_accuracy: 0.8274\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 26.7927 - accuracy: 0.7867 - val_loss: 16.2608 - val_accuracy: 0.8929\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 38.8868 - accuracy: 0.7994 - val_loss: 76.8875 - val_accuracy: 0.7411\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 40.0105 - accuracy: 0.8016 - val_loss: 19.3173 - val_accuracy: 0.8899\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 15.4879 - accuracy: 0.8084 - val_loss: 14.1927 - val_accuracy: 0.8899\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 13.0673 - accuracy: 0.8091 - val_loss: 9.7372 - val_accuracy: 0.8839\n",
      "Epoch 8/10\n",
      "23/42 [===============>..............] - ETA: 0s - loss: 20.6419 - accuracy: 0.8043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-01-21 21:22:02,360]\u001b[0m Trial 0 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\ryans\\AppData\\Local\\Temp\\ipykernel_15160\\3881561340.py\", line 37, in objective_v3\n",
      "    history = horsepower_model.fit(\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1606, in fit\n",
      "    val_logs = self.evaluate(\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1947, in evaluate\n",
      "    tmp_logs = self.test_function(iterator)\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 915, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 954, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2496, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1862, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 499, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"D:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mTPESampler(), study_name\u001b[38;5;241m=\u001b[39mobjective)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_v3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[31], line 37\u001b[0m, in \u001b[0;36mobjective_v3\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     horsepower_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     34\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     35\u001b[0m     )\n\u001b[1;32m---> 37\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mhorsepower_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Suppress logging.\u001b[39;49;00m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     oof[valid_idx] \u001b[38;5;241m=\u001b[39m horsepower_model\u001b[38;5;241m.\u001b[39mpredict(X_valid)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(Y, oof)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\source\\repos\\venv\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name=objective)\n",
    "study.optimize(objective_v3, timeout=train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093c8cec-58aa-40ed-ba64-047d7a38f90c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 1s 6ms/step - loss: 393.1921 - accuracy: 0.7942 - val_loss: 11.0909 - val_accuracy: 0.6250\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 30.7881 - accuracy: 0.7860 - val_loss: 16.1888 - val_accuracy: 0.7708\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 20.3969 - accuracy: 0.7875 - val_loss: 4.6991 - val_accuracy: 0.7857\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 20.8214 - accuracy: 0.7793 - val_loss: 17.9223 - val_accuracy: 0.8899\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 14.0799 - accuracy: 0.8061 - val_loss: 3.7521 - val_accuracy: 0.8869\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 7.3073 - accuracy: 0.7897 - val_loss: 6.6587 - val_accuracy: 0.8571\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 7.3433 - accuracy: 0.7987 - val_loss: 3.7537 - val_accuracy: 0.7292\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 6.2806 - accuracy: 0.7882 - val_loss: 4.9454 - val_accuracy: 0.3304\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 4.9615 - accuracy: 0.7964 - val_loss: 5.9512 - val_accuracy: 0.6845\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 6.3371 - accuracy: 0.7927 - val_loss: 3.6618 - val_accuracy: 0.8899\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 5.4088 - accuracy: 0.8024 - val_loss: 4.5188 - val_accuracy: 0.8899\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 3.8697 - accuracy: 0.8024 - val_loss: 7.4770 - val_accuracy: 0.8899\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 7.5619 - accuracy: 0.8009 - val_loss: 4.8362 - val_accuracy: 0.8155\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 3.4235 - accuracy: 0.8024 - val_loss: 1.1420 - val_accuracy: 0.8482\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 2.8542 - accuracy: 0.7934 - val_loss: 2.6715 - val_accuracy: 0.8601\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 3.1891 - accuracy: 0.8046 - val_loss: 1.4851 - val_accuracy: 0.8304\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 3.3455 - accuracy: 0.7890 - val_loss: 7.1386 - val_accuracy: 0.8899\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 3.1600 - accuracy: 0.8098 - val_loss: 1.6412 - val_accuracy: 0.8363\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 2.1681 - accuracy: 0.8076 - val_loss: 1.7369 - val_accuracy: 0.8869\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 3.0386 - accuracy: 0.7979 - val_loss: 1.8956 - val_accuracy: 0.8869\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 2.3651 - accuracy: 0.8069 - val_loss: 2.6780 - val_accuracy: 0.8899\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 1.9779 - accuracy: 0.8076 - val_loss: 2.8991 - val_accuracy: 0.8899\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 2.5834 - accuracy: 0.8151 - val_loss: 1.3126 - val_accuracy: 0.8839\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 2.6034 - accuracy: 0.8143 - val_loss: 3.5716 - val_accuracy: 0.6071\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 2.3888 - accuracy: 0.8076 - val_loss: 0.8058 - val_accuracy: 0.8869\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 1.7419 - accuracy: 0.8113 - val_loss: 2.0497 - val_accuracy: 0.5179\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 1.2238 - accuracy: 0.8143 - val_loss: 3.0320 - val_accuracy: 0.3274\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 1.2081 - accuracy: 0.8098 - val_loss: 0.7597 - val_accuracy: 0.8869\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.9728 - accuracy: 0.8218 - val_loss: 0.8950 - val_accuracy: 0.8661\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.9034 - accuracy: 0.8270 - val_loss: 0.9815 - val_accuracy: 0.5893\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.8782 - accuracy: 0.8203 - val_loss: 1.0260 - val_accuracy: 0.8571\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 1.1056 - accuracy: 0.8195 - val_loss: 1.1357 - val_accuracy: 0.8899\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.9018 - accuracy: 0.8285 - val_loss: 0.5014 - val_accuracy: 0.8869\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.8884 - accuracy: 0.8098 - val_loss: 0.5533 - val_accuracy: 0.8869\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.8553 - accuracy: 0.8173 - val_loss: 1.0722 - val_accuracy: 0.8899\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.9789 - accuracy: 0.8136 - val_loss: 0.4022 - val_accuracy: 0.8869\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.6688 - accuracy: 0.8374 - val_loss: 0.5785 - val_accuracy: 0.8869\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.9423 - accuracy: 0.8307 - val_loss: 2.1365 - val_accuracy: 0.8899\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.8101 - accuracy: 0.8292 - val_loss: 0.4214 - val_accuracy: 0.8869\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.7110 - accuracy: 0.8218 - val_loss: 0.3635 - val_accuracy: 0.8810\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.8273 - accuracy: 0.8292 - val_loss: 0.6670 - val_accuracy: 0.8869\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.7289 - accuracy: 0.8292 - val_loss: 0.3928 - val_accuracy: 0.8601\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.6203 - accuracy: 0.8248 - val_loss: 0.3988 - val_accuracy: 0.8839\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.7635 - accuracy: 0.8180 - val_loss: 0.5408 - val_accuracy: 0.8869\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.7976 - accuracy: 0.8240 - val_loss: 0.4753 - val_accuracy: 0.8869\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.5668 - accuracy: 0.8404 - val_loss: 0.5437 - val_accuracy: 0.8899\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5808 - accuracy: 0.8471 - val_loss: 0.3328 - val_accuracy: 0.8869\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.8412 - val_loss: 0.3169 - val_accuracy: 0.8839\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5746 - accuracy: 0.8382 - val_loss: 0.8650 - val_accuracy: 0.5298\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5931 - accuracy: 0.8434 - val_loss: 0.3594 - val_accuracy: 0.8869\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5251 - accuracy: 0.8516 - val_loss: 0.3403 - val_accuracy: 0.8899\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4914 - accuracy: 0.8434 - val_loss: 0.7056 - val_accuracy: 0.8899\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.6747 - accuracy: 0.8307 - val_loss: 0.6120 - val_accuracy: 0.7887\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 0.8434 - val_loss: 0.4556 - val_accuracy: 0.8601\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.8628 - val_loss: 0.3569 - val_accuracy: 0.8839\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8576 - val_loss: 0.3501 - val_accuracy: 0.8869\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5164 - accuracy: 0.8531 - val_loss: 0.3386 - val_accuracy: 0.8839\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.4519 - accuracy: 0.8583 - val_loss: 0.3362 - val_accuracy: 0.8839\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.7362 - accuracy: 0.8389 - val_loss: 0.4504 - val_accuracy: 0.8452\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4764 - accuracy: 0.8516 - val_loss: 0.3093 - val_accuracy: 0.8839\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.8606 - val_loss: 0.3552 - val_accuracy: 0.8869\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.8665 - val_loss: 0.3552 - val_accuracy: 0.8869\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.6002 - accuracy: 0.8337 - val_loss: 0.3959 - val_accuracy: 0.8601\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.4706 - accuracy: 0.8583 - val_loss: 0.4515 - val_accuracy: 0.8661\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4729 - accuracy: 0.8501 - val_loss: 0.3046 - val_accuracy: 0.8839\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8635 - val_loss: 0.3301 - val_accuracy: 0.8869\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4427 - accuracy: 0.8643 - val_loss: 0.3180 - val_accuracy: 0.8839\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4210 - accuracy: 0.8598 - val_loss: 0.3129 - val_accuracy: 0.8869\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4026 - accuracy: 0.8725 - val_loss: 0.3157 - val_accuracy: 0.8810\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.8553 - val_loss: 0.3165 - val_accuracy: 0.8839\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4406 - accuracy: 0.8561 - val_loss: 0.4821 - val_accuracy: 0.8869\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.8620 - val_loss: 0.3836 - val_accuracy: 0.8512\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.8576 - val_loss: 0.4948 - val_accuracy: 0.8036\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.8643 - val_loss: 0.3104 - val_accuracy: 0.8839\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.4530 - accuracy: 0.8538 - val_loss: 0.3386 - val_accuracy: 0.8839\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8688 - val_loss: 0.5052 - val_accuracy: 0.7887\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8598 - val_loss: 0.3046 - val_accuracy: 0.8839\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3894 - accuracy: 0.8658 - val_loss: 0.3402 - val_accuracy: 0.8869\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3941 - accuracy: 0.8732 - val_loss: 0.3077 - val_accuracy: 0.8839\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3938 - accuracy: 0.8695 - val_loss: 0.4126 - val_accuracy: 0.8274\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4041 - accuracy: 0.8673 - val_loss: 0.3392 - val_accuracy: 0.8482\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8613 - val_loss: 0.3388 - val_accuracy: 0.8869\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4675 - accuracy: 0.8576 - val_loss: 0.3835 - val_accuracy: 0.8869\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4080 - accuracy: 0.8732 - val_loss: 0.4425 - val_accuracy: 0.7976\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.8673 - val_loss: 0.3568 - val_accuracy: 0.8750\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8688 - val_loss: 0.3180 - val_accuracy: 0.8869\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8695 - val_loss: 0.3252 - val_accuracy: 0.8720\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8740 - val_loss: 0.3203 - val_accuracy: 0.8869\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8725 - val_loss: 0.3240 - val_accuracy: 0.8869\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8762 - val_loss: 0.3194 - val_accuracy: 0.8869\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8732 - val_loss: 0.3545 - val_accuracy: 0.8720\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8784 - val_loss: 0.2983 - val_accuracy: 0.8839\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8777 - val_loss: 0.3107 - val_accuracy: 0.8869\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4148 - accuracy: 0.8673 - val_loss: 0.3535 - val_accuracy: 0.8690\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8725 - val_loss: 0.3150 - val_accuracy: 0.8869\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3673 - accuracy: 0.8777 - val_loss: 0.3022 - val_accuracy: 0.8839\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8755 - val_loss: 0.3331 - val_accuracy: 0.8839\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3750 - accuracy: 0.8762 - val_loss: 0.3050 - val_accuracy: 0.8869\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8770 - val_loss: 0.3200 - val_accuracy: 0.8839\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3745 - accuracy: 0.8747 - val_loss: 0.4015 - val_accuracy: 0.8601\n",
      "11/11 [==============================] - 0s 1000us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.70306814], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df[features], train_df[target], test_size=0.20)\n",
    "horsepower_model = tf.keras.Sequential([\n",
    "    layers.Dense(1000, activation=\"relu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(1000, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "#reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "horsepower_model.compile(\n",
    "    loss='binary_crossentropy', optimizer='adam',metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "history = horsepower_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "\n",
    "    epochs=100,\n",
    "    # Suppress logging.\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "horsepower_model.predict(X_valid)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8954fca-0658-417d-b3c3-19a5a3265c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tfregressor'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ec511be-b75a-4fb3-8206-22e454d2bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "train_preds = []\n",
    "train_preds.append(horsepower_model.predict(train_df[features]))\n",
    "train_final_preds = np.stack(train_preds).mean(0)\n",
    "\n",
    "train_submission = pd.DataFrame(data={'id': train_df.index, target[0]: train_final_preds.reshape(len(train_final_preds)).tolist()})\n",
    "train_submission.to_csv(fr'rendered_data/{objective}_train_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e760c76c-fdbf-4f06-9300-6529f6c8a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 1000us/step\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "test_preds.append(horsepower_model.predict(test_df[features]))\n",
    "test_final_preds = np.stack(test_preds).mean(0)\n",
    "\n",
    "test_submission = pd.DataFrame(data={'id': test_df.index, target[0]: test_final_preds.reshape(len(test_final_preds)).tolist()})\n",
    "test_submission.to_csv(fr'rendered_data/{objective}_test_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9714d0b2-757a-432e-a810-40b5d0df67b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29763534665107727,\n",
       " 0.2145659178495407,\n",
       " 0.14717520773410797,\n",
       " 0.1498907506465912,\n",
       " 0.4386022388935089,\n",
       " 0.29969337582588196,\n",
       " 0.5491160750389099,\n",
       " 0.17812363803386688,\n",
       " 0.08284812420606613,\n",
       " 0.2441578060388565,\n",
       " 0.20705869793891907,\n",
       " 0.27838873863220215,\n",
       " 0.34607991576194763,\n",
       " 0.3098334074020386,\n",
       " 0.21122094988822937,\n",
       " 0.27135735750198364,\n",
       " 0.265879362821579,\n",
       " 0.13833941519260406,\n",
       " 0.1397162824869156,\n",
       " 0.11753559857606888,\n",
       " 0.4148927628993988,\n",
       " 0.269684761762619,\n",
       " 0.41575735807418823,\n",
       " 0.2872975468635559,\n",
       " 0.21741734445095062,\n",
       " 0.20274662971496582,\n",
       " 0.3006517291069031,\n",
       " 0.23984336853027344,\n",
       " 0.2692815959453583,\n",
       " 0.20518939197063446,\n",
       " 0.0895732119679451,\n",
       " 0.07171088457107544,\n",
       " 0.23958809673786163,\n",
       " 0.0762794092297554,\n",
       " 0.17831914126873016,\n",
       " 0.3704150319099426,\n",
       " 0.39833006262779236,\n",
       " 0.22744838893413544,\n",
       " 0.11358622461557388,\n",
       " 0.8305570483207703,\n",
       " 0.3068463206291199,\n",
       " 0.24102213978767395,\n",
       " 0.40601611137390137,\n",
       " 0.307915598154068,\n",
       " 0.198457270860672,\n",
       " 0.07826673239469528,\n",
       " 0.34758460521698,\n",
       " 0.513454258441925,\n",
       " 0.5266005396842957,\n",
       " 0.18372842669487,\n",
       " 0.2277851402759552,\n",
       " 0.2226753979921341,\n",
       " 0.15908633172512054,\n",
       " 0.23621386289596558,\n",
       " 0.3808259665966034,\n",
       " 0.24444490671157837,\n",
       " 0.21314385533332825,\n",
       " 0.16615362465381622,\n",
       " 0.2531658113002777,\n",
       " 0.36204952001571655,\n",
       " 0.26727357506752014,\n",
       " 0.26152175664901733,\n",
       " 0.20452764630317688,\n",
       " 0.20028121769428253,\n",
       " 0.5708954930305481,\n",
       " 0.11631561815738678,\n",
       " 0.24868881702423096,\n",
       " 0.27643725275993347,\n",
       " 0.11604999750852585,\n",
       " 0.22223615646362305,\n",
       " 0.17941343784332275,\n",
       " 0.19665013253688812,\n",
       " 0.21020273864269257,\n",
       " 0.1899050772190094,\n",
       " 0.7030681371688843,\n",
       " 0.4807186722755432,\n",
       " 0.3795652389526367,\n",
       " 0.17844261229038239,\n",
       " 0.36770451068878174,\n",
       " 0.17120543122291565,\n",
       " 0.23731960356235504,\n",
       " 0.29123035073280334,\n",
       " 0.2813875377178192,\n",
       " 0.2611578702926636,\n",
       " 0.20412872731685638,\n",
       " 0.31052422523498535,\n",
       " 0.08893243968486786,\n",
       " 0.24355344474315643,\n",
       " 0.10047030448913574,\n",
       " 0.3162071704864502,\n",
       " 0.8813926577568054,\n",
       " 0.24889430403709412,\n",
       " 0.40517714619636536,\n",
       " 0.04957412928342819,\n",
       " 0.421972393989563,\n",
       " 0.21726787090301514,\n",
       " 0.45745766162872314,\n",
       " 0.22505667805671692,\n",
       " 0.15187089145183563,\n",
       " 0.08418111503124237,\n",
       " 0.2228749692440033,\n",
       " 0.19918476045131683,\n",
       " 0.4169752597808838,\n",
       " 0.09699930995702744,\n",
       " 0.5184279680252075,\n",
       " 0.2640868127346039,\n",
       " 0.2035810798406601,\n",
       " 0.17402437329292297,\n",
       " 0.28072789311408997,\n",
       " 0.32613494992256165,\n",
       " 0.8028972744941711,\n",
       " 0.2990545630455017,\n",
       " 0.46055158972740173,\n",
       " 0.09999871999025345,\n",
       " 0.2341017723083496,\n",
       " 0.17735110223293304,\n",
       " 0.18220146000385284,\n",
       " 0.46442556381225586,\n",
       " 0.45938634872436523,\n",
       " 0.1063760295510292,\n",
       " 0.16640128195285797,\n",
       " 0.2965095043182373,\n",
       " 0.19408607482910156,\n",
       " 0.16389790177345276,\n",
       " 0.1278953105211258,\n",
       " 0.09354446828365326,\n",
       " 0.0955193042755127,\n",
       " 0.12687911093235016,\n",
       " 0.3385416269302368,\n",
       " 0.09883614629507065,\n",
       " 0.15297454595565796,\n",
       " 0.12467258423566818,\n",
       " 0.29523757100105286,\n",
       " 0.13693395256996155,\n",
       " 0.2817395031452179,\n",
       " 0.26309463381767273,\n",
       " 0.19938822090625763,\n",
       " 0.16267001628875732,\n",
       " 0.23709765076637268,\n",
       " 0.13815821707248688,\n",
       " 0.25174129009246826,\n",
       " 0.24889984726905823,\n",
       " 0.25447744131088257,\n",
       " 0.12706059217453003,\n",
       " 0.2570190727710724,\n",
       " 0.46382027864456177,\n",
       " 0.2701225280761719,\n",
       " 0.1051754280924797,\n",
       " 0.2502446472644806,\n",
       " 0.3026326596736908,\n",
       " 0.26212283968925476,\n",
       " 0.09106601774692535,\n",
       " 0.23125268518924713,\n",
       " 0.14909662306308746,\n",
       " 0.16463030874729156,\n",
       " 0.3653852343559265,\n",
       " 0.12160515785217285,\n",
       " 0.2926334738731384,\n",
       " 0.2246912270784378,\n",
       " 0.30787140130996704,\n",
       " 0.10347380489110947,\n",
       " 0.3596404492855072,\n",
       " 0.1792411506175995,\n",
       " 0.11464241147041321,\n",
       " 0.06200287491083145,\n",
       " 0.3133310377597809,\n",
       " 0.3054051399230957,\n",
       " 0.7957742214202881,\n",
       " 0.4811081290245056,\n",
       " 0.23439180850982666,\n",
       " 0.15791305899620056,\n",
       " 0.18841123580932617,\n",
       " 0.21017156541347504,\n",
       " 0.08374226838350296,\n",
       " 0.22516107559204102,\n",
       " 0.13939814269542694,\n",
       " 0.160256028175354,\n",
       " 0.17409181594848633,\n",
       " 0.6697084307670593,\n",
       " 0.23009054362773895,\n",
       " 0.15186645090579987,\n",
       " 0.3088468909263611,\n",
       " 0.2924797832965851,\n",
       " 0.48636072874069214,\n",
       " 0.24639776349067688,\n",
       " 0.2722499966621399,\n",
       " 0.21017944812774658,\n",
       " 0.12665334343910217,\n",
       " 0.25894591212272644,\n",
       " 0.0878409743309021,\n",
       " 0.31850773096084595,\n",
       " 0.25453171133995056,\n",
       " 0.3633415400981903,\n",
       " 0.11499637365341187,\n",
       " 0.21249423921108246,\n",
       " 0.12519441545009613,\n",
       " 0.2365720421075821,\n",
       " 0.3498515486717224,\n",
       " 0.41973966360092163,\n",
       " 0.1934034675359726,\n",
       " 0.30922967195510864,\n",
       " 0.08325668424367905,\n",
       " 0.28213852643966675,\n",
       " 0.3093642294406891,\n",
       " 0.3026144206523895,\n",
       " 0.35663604736328125,\n",
       " 0.2486661672592163,\n",
       " 0.157352015376091,\n",
       " 0.3070650100708008,\n",
       " 0.3009021580219269,\n",
       " 0.09596291929483414,\n",
       " 0.11147651821374893,\n",
       " 0.19585271179676056,\n",
       " 0.24494557082653046,\n",
       " 0.06399599462747574,\n",
       " 0.17780062556266785,\n",
       " 0.1602681279182434,\n",
       " 0.5658587217330933,\n",
       " 0.11030804365873337,\n",
       " 0.10761525481939316,\n",
       " 0.2833893299102783,\n",
       " 0.2986327111721039,\n",
       " 0.14996621012687683,\n",
       " 0.2835252285003662,\n",
       " 0.15091460943222046,\n",
       " 0.2830863296985626,\n",
       " 0.3036762773990631,\n",
       " 0.26439666748046875,\n",
       " 0.27604496479034424,\n",
       " 0.27590975165367126,\n",
       " 0.3826529383659363,\n",
       " 0.13904263079166412,\n",
       " 0.12012580782175064,\n",
       " 0.3887462019920349,\n",
       " 0.2596270740032196,\n",
       " 0.20208445191383362,\n",
       " 0.12126699090003967,\n",
       " 0.23086853325366974,\n",
       " 0.14415088295936584,\n",
       " 0.11086151748895645,\n",
       " 0.2680375874042511,\n",
       " 0.2807773947715759,\n",
       " 0.13154201209545135,\n",
       " 0.21330951154232025,\n",
       " 0.3544759452342987,\n",
       " 0.23251159489154816,\n",
       " 0.2926197946071625,\n",
       " 0.15683826804161072,\n",
       " 0.22147440910339355,\n",
       " 0.23038983345031738,\n",
       " 0.800896942615509,\n",
       " 0.13752584159374237,\n",
       " 0.16305914521217346,\n",
       " 0.1697588711977005,\n",
       " 0.3537028729915619,\n",
       " 0.09227339178323746,\n",
       " 0.309486448764801,\n",
       " 0.18981774151325226,\n",
       " 0.2670682966709137,\n",
       " 0.12390399724245071,\n",
       " 0.43911173939704895,\n",
       " 0.3286416828632355,\n",
       " 0.13239386677742004,\n",
       " 0.35412439703941345,\n",
       " 0.3538053035736084,\n",
       " 0.47865796089172363,\n",
       " 0.10858742892742157,\n",
       " 0.12025845050811768,\n",
       " 0.2663361728191376,\n",
       " 0.11527008563280106,\n",
       " 0.3578772246837616,\n",
       " 0.470216304063797,\n",
       " 0.3583540916442871,\n",
       " 0.09516946226358414,\n",
       " 0.2758346199989319,\n",
       " 0.06825133413076401,\n",
       " 0.11058328300714493,\n",
       " 0.08558931201696396,\n",
       " 0.2189566195011139,\n",
       " 0.11414084583520889,\n",
       " 0.11506057530641556,\n",
       " 0.23032216727733612,\n",
       " 0.2276209145784378,\n",
       " 0.24028056859970093,\n",
       " 0.25742167234420776,\n",
       " 0.08177446573972702,\n",
       " 0.30221042037010193,\n",
       " 0.1770346760749817,\n",
       " 0.15035420656204224,\n",
       " 0.0021207064855843782,\n",
       " 0.2601985037326813,\n",
       " 0.2472240924835205,\n",
       " 0.19091014564037323,\n",
       " 0.11495885252952576,\n",
       " 0.14033976197242737,\n",
       " 0.27243924140930176,\n",
       " 0.15266172587871552,\n",
       " 0.36185702681541443,\n",
       " 0.26503613591194153,\n",
       " 0.2527649402618408,\n",
       " 0.11782234907150269,\n",
       " 0.2587004005908966,\n",
       " 0.2683180272579193,\n",
       " 0.3637233376502991,\n",
       " 0.07964516431093216,\n",
       " 0.17398056387901306,\n",
       " 0.42094841599464417,\n",
       " 0.2216493785381317,\n",
       " 0.18392892181873322,\n",
       " 0.25777503848075867,\n",
       " 0.24938006699085236,\n",
       " 0.12439301609992981,\n",
       " 0.18102605640888214,\n",
       " 0.3943599760532379,\n",
       " 0.2493794560432434,\n",
       " 0.07581817358732224,\n",
       " 0.2141963541507721,\n",
       " 0.098138727247715,\n",
       " 0.2389036864042282,\n",
       " 0.08768266439437866,\n",
       " 0.3269972801208496,\n",
       " 0.3474074602127075,\n",
       " 0.1502169370651245,\n",
       " 0.13357453048229218,\n",
       " 0.2748067080974579,\n",
       " 0.09100569039583206,\n",
       " 0.12656329572200775,\n",
       " 0.2721911072731018,\n",
       " 0.29760080575942993,\n",
       " 0.21254174411296844,\n",
       " 0.2672993540763855,\n",
       " 0.2548307478427887,\n",
       " 0.3127259910106659,\n",
       " 0.3229151666164398,\n",
       " 0.2263263761997223,\n",
       " 0.3760174512863159,\n",
       " 0.13687679171562195,\n",
       " 0.27054086327552795,\n",
       " 0.30437928438186646,\n",
       " 0.18449591100215912,\n",
       " 0.12392092496156693,\n",
       " 0.14268465340137482,\n",
       " 0.1868574321269989,\n",
       " 0.23181141912937164,\n",
       " 0.8565502166748047,\n",
       " 0.1697409749031067,\n",
       " 0.3323397934436798,\n",
       " 0.19739395380020142,\n",
       " 0.142790749669075,\n",
       " 0.37755101919174194,\n",
       " 0.27839797735214233,\n",
       " 0.09279309958219528,\n",
       " 0.19349433481693268,\n",
       " 0.3268987834453583,\n",
       " 0.19713282585144043,\n",
       " 0.2778731882572174,\n",
       " 0.2668011784553528,\n",
       " 0.2968910038471222,\n",
       " 0.12686963379383087,\n",
       " 0.11243806034326553,\n",
       " 0.2599327266216278,\n",
       " 0.4266309142112732,\n",
       " 0.24412961304187775,\n",
       " 0.27883392572402954,\n",
       " 0.15059132874011993,\n",
       " 0.2847791314125061,\n",
       " 0.3415478765964508,\n",
       " 0.4261990785598755,\n",
       " 0.2921154797077179,\n",
       " 0.2251807004213333,\n",
       " 0.14090017974376678,\n",
       " 0.11901268362998962,\n",
       " 0.31751978397369385,\n",
       " 0.0009287697612307966,\n",
       " 0.502063512802124,\n",
       " 0.17568592727184296,\n",
       " 0.4429357945919037,\n",
       " 0.23870058357715607,\n",
       " 0.20706698298454285,\n",
       " 0.13017933070659637,\n",
       " 0.36058399081230164,\n",
       " 0.22497013211250305,\n",
       " 0.3225228488445282,\n",
       " 0.2594861388206482,\n",
       " 0.1477207988500595,\n",
       " 0.103936105966568,\n",
       " 0.5775037407875061,\n",
       " 0.7432186603546143,\n",
       " 0.1913175731897354,\n",
       " 0.27159491181373596,\n",
       " 0.13971978425979614,\n",
       " 0.22382986545562744,\n",
       " 0.25019145011901855,\n",
       " 0.3066055476665497,\n",
       " 0.1843794733285904,\n",
       " 0.06480614095926285,\n",
       " 0.27617913484573364,\n",
       " 0.20259983837604523,\n",
       " 0.19440187513828278,\n",
       " 0.08762262761592865,\n",
       " 0.20372483134269714,\n",
       " 0.09087781608104706,\n",
       " 0.19553609192371368,\n",
       " 0.25523868203163147,\n",
       " 0.22129450738430023,\n",
       " 0.5963925123214722,\n",
       " 0.23020753264427185,\n",
       " 0.24520792067050934,\n",
       " 0.2740240693092346,\n",
       " 0.13411536812782288,\n",
       " 0.20014308393001556,\n",
       " 0.11187966167926788,\n",
       " 0.453736275434494,\n",
       " 0.2577153146266937,\n",
       " 0.09325870126485825,\n",
       " 0.2317066639661789,\n",
       " 0.6488802433013916,\n",
       " 0.15935537219047546,\n",
       " 0.06248415634036064,\n",
       " 0.21413356065750122,\n",
       " 0.2097470760345459,\n",
       " 0.4949146807193756,\n",
       " 0.1385323703289032,\n",
       " 0.18564145267009735,\n",
       " 0.21473270654678345,\n",
       " 0.2609557509422302,\n",
       " 0.26861631870269775,\n",
       " 0.06610267609357834,\n",
       " 0.10087711364030838,\n",
       " 0.21992415189743042,\n",
       " 0.21925243735313416,\n",
       " 0.42504963278770447,\n",
       " 0.2482631951570511,\n",
       " 0.2209842950105667,\n",
       " 0.15121327340602875,\n",
       " 0.15977488458156586,\n",
       " 0.2761094272136688,\n",
       " 0.3756980895996094,\n",
       " 0.2272757589817047,\n",
       " 0.3767157196998596,\n",
       " 0.33174943923950195,\n",
       " 0.1718090921640396,\n",
       " 0.1516842395067215,\n",
       " 0.28688153624534607,\n",
       " 0.10892652720212936,\n",
       " 0.13329242169857025,\n",
       " 0.24984481930732727,\n",
       " 0.22462408244609833,\n",
       " 0.11797747015953064,\n",
       " 0.33730053901672363,\n",
       " 0.2907067537307739,\n",
       " 0.30891454219818115,\n",
       " 0.4766952693462372,\n",
       " 0.27063989639282227,\n",
       " 0.21494467556476593,\n",
       " 0.21500042080879211,\n",
       " 0.1629418432712555,\n",
       " 0.19898611307144165,\n",
       " 0.1241249144077301,\n",
       " 0.13452167809009552,\n",
       " 0.25020596385002136,\n",
       " 0.1775071918964386,\n",
       " 0.19169266521930695,\n",
       " 0.14278224110603333,\n",
       " 0.21551160514354706,\n",
       " 0.21064215898513794,\n",
       " 0.2677542269229889,\n",
       " 0.1625172197818756,\n",
       " 0.13575418293476105,\n",
       " 0.2153884470462799,\n",
       " 0.12445013225078583,\n",
       " 0.32653361558914185,\n",
       " 0.2795431613922119,\n",
       " 0.2761267423629761,\n",
       " 0.3084747791290283,\n",
       " 0.38071706891059875,\n",
       " 0.2695314288139343,\n",
       " 0.27335721254348755,\n",
       " 0.21769791841506958,\n",
       " 0.21640180051326752,\n",
       " 0.32007962465286255,\n",
       " 0.14255008101463318,\n",
       " 0.27473440766334534,\n",
       " 0.3160111606121063,\n",
       " 0.07230924814939499,\n",
       " 0.20759396255016327,\n",
       " 0.2710917294025421,\n",
       " 0.11430849879980087,\n",
       " 0.1760302633047104,\n",
       " 0.31420809030532837,\n",
       " 0.34257808327674866,\n",
       " 0.18312302231788635,\n",
       " 0.26439163088798523,\n",
       " 0.5870153903961182,\n",
       " 0.2983352839946747,\n",
       " 0.3172069787979126,\n",
       " 0.3166641294956207,\n",
       " 0.21220792829990387,\n",
       " 0.22691401839256287,\n",
       " 0.41967737674713135,\n",
       " 0.3251664936542511,\n",
       " 0.2661874294281006,\n",
       " 0.14299212396144867,\n",
       " 0.3653270900249481,\n",
       " 0.1660986691713333,\n",
       " 0.3697275221347809,\n",
       " 0.3699634075164795,\n",
       " 0.20169930160045624,\n",
       " 0.2279588133096695,\n",
       " 0.24340543150901794,\n",
       " 0.28650084137916565,\n",
       " 0.1287258416414261,\n",
       " 0.1884407252073288,\n",
       " 0.01071513257920742,\n",
       " 0.19761016964912415,\n",
       " 0.26082292199134827,\n",
       " 0.14418703317642212,\n",
       " 0.1420026570558548,\n",
       " 0.0829349011182785,\n",
       " 0.3136827051639557,\n",
       " 0.14876122772693634,\n",
       " 0.2548658847808838,\n",
       " 0.12146463245153427,\n",
       " 0.12602518498897552,\n",
       " 0.09212058782577515,\n",
       " 0.4308386743068695,\n",
       " 0.15955188870429993,\n",
       " 0.27915653586387634,\n",
       " 0.21220122277736664,\n",
       " 0.4838833212852478,\n",
       " 0.35417160391807556,\n",
       " 0.2632685899734497,\n",
       " 0.3715737760066986,\n",
       " 0.14178720116615295,\n",
       " 0.43847429752349854,\n",
       " 0.16339732706546783,\n",
       " 0.1580704152584076,\n",
       " 0.1692667007446289,\n",
       " 0.13648180663585663,\n",
       " 0.11042175441980362,\n",
       " 0.2437102198600769,\n",
       " 0.2953025996685028,\n",
       " 0.1701187789440155,\n",
       " 0.24512045085430145,\n",
       " 0.16412149369716644,\n",
       " 0.31265681982040405,\n",
       " 0.2740651071071625,\n",
       " 0.23796062171459198,\n",
       " 0.1597721427679062,\n",
       " 0.26895782351493835,\n",
       " 0.33188575506210327,\n",
       " 0.14758972823619843,\n",
       " 0.39586541056632996,\n",
       " 0.05934363976120949,\n",
       " 0.013848025351762772,\n",
       " 0.0936732217669487,\n",
       " 0.12795566022396088,\n",
       " 0.5944274067878723,\n",
       " 0.2984197437763214,\n",
       " 0.13153226673603058,\n",
       " 0.281795859336853,\n",
       " 0.23451638221740723,\n",
       " 0.1432162970304489,\n",
       " 0.3476381301879883,\n",
       " 0.056580208241939545,\n",
       " 0.30361267924308777,\n",
       " 0.22317402064800262,\n",
       " 0.2649262845516205,\n",
       " 0.12119424343109131,\n",
       " 0.23518730700016022,\n",
       " 0.17281752824783325,\n",
       " 0.30149930715560913,\n",
       " 0.1620829850435257,\n",
       " 0.308966726064682,\n",
       " 0.2054056078195572,\n",
       " 0.24958622455596924,\n",
       " 0.22364625334739685,\n",
       " 0.3010619878768921,\n",
       " 0.31903645396232605,\n",
       " 0.16724351048469543,\n",
       " 0.3177533745765686,\n",
       " 0.2854168713092804,\n",
       " 0.11069391667842865,\n",
       " 0.3739326298236847,\n",
       " 0.3113083243370056,\n",
       " 0.3212382197380066,\n",
       " 0.16682735085487366,\n",
       " 0.4960198402404785,\n",
       " 0.14515550434589386,\n",
       " 0.18938255310058594,\n",
       " 0.23746901750564575,\n",
       " 0.24807094037532806,\n",
       " 0.31824684143066406,\n",
       " 0.3292214870452881,\n",
       " 0.2670632302761078,\n",
       " 0.16257598996162415,\n",
       " 0.21833842992782593,\n",
       " 0.31680968403816223,\n",
       " 0.10709544271230698,\n",
       " 0.12776778638362885,\n",
       " 0.5644016265869141,\n",
       " 0.41566258668899536,\n",
       " 0.24237021803855896,\n",
       " 0.24949011206626892,\n",
       " 0.27953583002090454,\n",
       " 0.25898128747940063,\n",
       " 0.11843850463628769,\n",
       " 0.2796226143836975,\n",
       " 0.22635547816753387,\n",
       " 0.10803515464067459,\n",
       " 0.09913499653339386,\n",
       " 0.2570192515850067,\n",
       " 0.18175505101680756,\n",
       " 0.17389072477817535,\n",
       " 0.3240719735622406,\n",
       " 0.08627865463495255,\n",
       " 0.2775961756706238,\n",
       " 0.1972832828760147,\n",
       " 0.2814469635486603,\n",
       " 0.17283304035663605,\n",
       " 0.17312954366207123,\n",
       " 0.39185068011283875,\n",
       " 0.36815041303634644,\n",
       " 0.4001436233520508,\n",
       " 0.23933258652687073,\n",
       " 0.1716248244047165,\n",
       " 0.15315380692481995,\n",
       " 0.32706543803215027,\n",
       " 0.2838069796562195,\n",
       " 0.31982165575027466,\n",
       " 0.11855729669332504,\n",
       " 0.2556011378765106,\n",
       " 0.24489076435565948,\n",
       " 0.5400762557983398,\n",
       " 0.31313517689704895,\n",
       " 0.3118911683559418,\n",
       " 0.06379377841949463,\n",
       " 0.1645689755678177,\n",
       " 0.26052719354629517,\n",
       " 0.2187594622373581,\n",
       " 0.1903826892375946,\n",
       " 0.07709160447120667,\n",
       " 0.19393599033355713,\n",
       " 0.2605951428413391,\n",
       " 0.3033708333969116,\n",
       " 0.15256302058696747,\n",
       " 0.2557196617126465,\n",
       " 0.2766449749469757,\n",
       " 0.16790293157100677,\n",
       " 0.23430614173412323,\n",
       " 0.11655198037624359,\n",
       " 0.16226883232593536,\n",
       " 0.4265313744544983,\n",
       " 0.6975613236427307,\n",
       " 0.07415978610515594,\n",
       " 0.5414950847625732,\n",
       " 0.6683634519577026,\n",
       " 0.14805220067501068,\n",
       " 0.36611130833625793,\n",
       " 0.4863142669200897,\n",
       " 0.225045844912529,\n",
       " 0.13158026337623596,\n",
       " 0.24507954716682434,\n",
       " 0.47681087255477905,\n",
       " 0.3565431535243988,\n",
       " 0.2614798843860626,\n",
       " 0.5801416039466858,\n",
       " 0.14650283753871918,\n",
       " 0.2648240327835083,\n",
       " 0.40408143401145935,\n",
       " 0.16775666177272797,\n",
       " 0.23836280405521393,\n",
       " 0.2181437909603119,\n",
       " 0.39301639795303345,\n",
       " 0.30506592988967896,\n",
       " 0.4261930584907532,\n",
       " 0.32440391182899475,\n",
       " 0.6564674377441406,\n",
       " 0.5010191202163696,\n",
       " 0.17762112617492676,\n",
       " 0.20867516100406647,\n",
       " 0.2996281683444977,\n",
       " 0.28221434354782104,\n",
       " 0.1464187055826187,\n",
       " 0.3322461247444153,\n",
       " 0.29022711515426636,\n",
       " 0.3550403416156769,\n",
       " 0.20363107323646545,\n",
       " 0.1377543956041336,\n",
       " 0.21362674236297607,\n",
       " 0.2290961891412735,\n",
       " 0.1997358798980713,\n",
       " 0.07831576466560364,\n",
       " 0.24680018424987793,\n",
       " 0.12062550336122513,\n",
       " 0.0005827247514389455,\n",
       " 0.08764138072729111,\n",
       " 0.24471497535705566,\n",
       " 0.23292283713817596,\n",
       " 0.13190717995166779,\n",
       " 0.2145906537771225,\n",
       " 0.22774773836135864,\n",
       " 0.2888435423374176,\n",
       " 0.3706001341342926,\n",
       " 0.19152314960956573,\n",
       " 0.479954332113266,\n",
       " 0.0021478156559169292,\n",
       " 0.22414785623550415,\n",
       " 0.27989086508750916,\n",
       " 0.2944369316101074,\n",
       " 0.14456439018249512,\n",
       " 0.2084462195634842,\n",
       " 0.24746359884738922,\n",
       " 0.594180166721344,\n",
       " 0.43916094303131104,\n",
       " 0.4663870930671692,\n",
       " 0.07649208605289459,\n",
       " 0.30364298820495605,\n",
       " 0.3259831666946411,\n",
       " 0.35726022720336914,\n",
       " 0.26230543851852417,\n",
       " 0.14035649597644806,\n",
       " 0.29794231057167053,\n",
       " 0.19784557819366455,\n",
       " 0.2806531488895416,\n",
       " 0.23927335441112518,\n",
       " 0.26704731583595276,\n",
       " 0.1056547686457634,\n",
       " 0.3213065564632416,\n",
       " 0.29250308871269226,\n",
       " 0.06032582372426987,\n",
       " 0.2177315056324005,\n",
       " 0.31593742966651917,\n",
       " 0.10067803412675858,\n",
       " 0.1131667047739029,\n",
       " 0.00031250048778019845,\n",
       " 0.2750932276248932,\n",
       " 0.2938311696052551,\n",
       " 0.23440776765346527,\n",
       " 0.11706665903329849,\n",
       " 0.19546949863433838,\n",
       " 0.09611213207244873,\n",
       " 0.16607750952243805,\n",
       " 0.40249818563461304,\n",
       " 0.1558709293603897,\n",
       " 0.1482083797454834,\n",
       " 0.47838494181632996,\n",
       " 0.3398144543170929,\n",
       " 0.3058106601238251,\n",
       " 0.12411106377840042,\n",
       " 0.17752861976623535,\n",
       " 0.19013743102550507,\n",
       " 0.28597599267959595,\n",
       " 0.746572732925415,\n",
       " 0.34713461995124817,\n",
       " 0.280682772397995,\n",
       " 0.14938868582248688,\n",
       " 0.1453472077846527,\n",
       " 0.14940747618675232,\n",
       " 0.13990752398967743,\n",
       " 0.3370610177516937,\n",
       " 0.2227184921503067,\n",
       " 0.2495092898607254,\n",
       " 0.32858532667160034,\n",
       " 0.2996547520160675,\n",
       " 0.27346333861351013,\n",
       " 0.27796515822410583,\n",
       " 0.24842579662799835,\n",
       " 0.28262194991111755,\n",
       " 0.18992960453033447,\n",
       " 0.4775618016719818,\n",
       " 0.1973510980606079,\n",
       " 0.3914332687854767,\n",
       " 0.25524669885635376,\n",
       " 0.24648290872573853,\n",
       " 0.23923532664775848,\n",
       " 0.11344663053750992,\n",
       " 0.36774924397468567,\n",
       " 0.18319810926914215,\n",
       " 0.09780658036470413,\n",
       " 0.2499264031648636,\n",
       " 0.25943660736083984,\n",
       " 0.199242502450943,\n",
       " 0.17820172011852264,\n",
       " 0.11895047128200531,\n",
       " 0.12940864264965057,\n",
       " 0.13177284598350525,\n",
       " 0.44435930252075195,\n",
       " 0.11589039117097855,\n",
       " 0.13652965426445007,\n",
       " 0.20195670425891876,\n",
       " 0.19527475535869598,\n",
       " 0.22063565254211426,\n",
       " 0.08555560559034348,\n",
       " 0.7573119401931763,\n",
       " 0.2799791693687439,\n",
       " 0.24863651394844055,\n",
       " 0.365439236164093,\n",
       " 0.1708076447248459,\n",
       " 0.000946075888350606,\n",
       " 0.20086604356765747,\n",
       " 0.1198003739118576,\n",
       " 0.20856323838233948,\n",
       " 0.2845515012741089,\n",
       " 0.31622639298439026,\n",
       " 0.3913184702396393,\n",
       " 0.2933304011821747,\n",
       " 0.1513461023569107,\n",
       " 0.2377540022134781,\n",
       " 0.22500240802764893,\n",
       " 0.18102391064167023,\n",
       " 0.4916380047798157,\n",
       " 0.2854241132736206,\n",
       " 0.13519002497196198,\n",
       " 0.2936893105506897,\n",
       " 0.35810571908950806,\n",
       " 0.26509103178977966,\n",
       " 0.12061873078346252,\n",
       " 0.35120055079460144,\n",
       " 0.22899407148361206,\n",
       " 0.3741482198238373,\n",
       " 0.3238591253757477,\n",
       " 0.2020261585712433,\n",
       " 0.23804639279842377,\n",
       " 0.13215866684913635,\n",
       " 0.09265660494565964,\n",
       " 0.3101472854614258,\n",
       " 0.47732484340667725,\n",
       " 0.24006721377372742,\n",
       " 0.3567095696926117,\n",
       " 0.2972181439399719,\n",
       " 0.31229591369628906,\n",
       " 0.2715681493282318,\n",
       " 0.08938869088888168,\n",
       " 0.17052985727787018,\n",
       " 0.1578516811132431,\n",
       " 0.277304083108902,\n",
       " 0.2141476273536682,\n",
       " 0.16130876541137695,\n",
       " 0.15015089511871338,\n",
       " 0.18649330735206604,\n",
       " 0.08973522484302521,\n",
       " 0.3127322793006897,\n",
       " 0.5224317908287048,\n",
       " 0.1375173032283783,\n",
       " 0.2719557285308838,\n",
       " 0.2519088685512543,\n",
       " 9.921022865455598e-07,\n",
       " 0.3002641499042511,\n",
       " 0.22397123277187347,\n",
       " 0.09224752336740494,\n",
       " 0.2523760497570038,\n",
       " 0.1574656367301941,\n",
       " 0.17828269302845,\n",
       " 0.19439449906349182,\n",
       " 0.36903566122055054,\n",
       " 0.2170415222644806,\n",
       " 0.15799906849861145,\n",
       " 0.1344546526670456,\n",
       " 0.3645923435688019,\n",
       " 0.18843331933021545,\n",
       " 0.2673821747303009,\n",
       " 0.5336758494377136,\n",
       " 0.2458341270685196,\n",
       " 0.4058828353881836,\n",
       " 0.27967122197151184,\n",
       " 0.14436203241348267,\n",
       " 0.29903122782707214,\n",
       " 0.08777512609958649,\n",
       " 0.17252203822135925,\n",
       " 0.20745661854743958,\n",
       " 0.2778307795524597,\n",
       " 0.12695859372615814,\n",
       " 0.23216083645820618,\n",
       " 0.2288029044866562,\n",
       " 0.16739009320735931,\n",
       " 0.19749096035957336,\n",
       " 0.1367904245853424,\n",
       " 0.2601202130317688,\n",
       " 0.2613787353038788,\n",
       " 0.5808262228965759,\n",
       " 0.26338422298431396,\n",
       " 0.27882876992225647,\n",
       " 0.2852571904659271,\n",
       " 0.4173372983932495,\n",
       " 0.1812315434217453,\n",
       " 0.19814255833625793,\n",
       " 0.17739051580429077,\n",
       " 0.26977992057800293,\n",
       " 0.18340277671813965,\n",
       " 0.5150271654129028,\n",
       " 0.08204159140586853,\n",
       " 0.2421257644891739,\n",
       " 0.291212797164917,\n",
       " 0.4764251410961151,\n",
       " 0.2343556135892868,\n",
       " 0.04451639577746391,\n",
       " 0.2005303055047989,\n",
       " 0.25217312574386597,\n",
       " 0.5308579206466675,\n",
       " 0.36519938707351685,\n",
       " 0.2649696171283722,\n",
       " 0.1327776163816452,\n",
       " 0.16873548924922943,\n",
       " 0.28924262523651123,\n",
       " 0.2907662093639374,\n",
       " 0.1984618455171585,\n",
       " 0.24712218344211578,\n",
       " 0.14228029549121857,\n",
       " 0.08068575710058212,\n",
       " 0.25300824642181396,\n",
       " 0.1574932485818863,\n",
       " 0.29097428917884827,\n",
       " 0.09061971306800842,\n",
       " 0.17562375962734222,\n",
       " 0.15580904483795166,\n",
       " 0.31011319160461426,\n",
       " 0.17506681382656097,\n",
       " 0.3936336934566498,\n",
       " 0.21282202005386353,\n",
       " 0.4303094446659088,\n",
       " 0.0967692956328392,\n",
       " 0.25195103883743286,\n",
       " 0.3263345956802368,\n",
       " 0.16603125631809235,\n",
       " 0.13851158320903778,\n",
       " 0.2232344150543213,\n",
       " 0.22464419901371002,\n",
       " 0.29060956835746765,\n",
       " 0.36081957817077637,\n",
       " 0.31915411353111267,\n",
       " 0.15664644539356232,\n",
       " 0.13072390854358673,\n",
       " 0.1529334932565689,\n",
       " 0.536112368106842,\n",
       " 0.48916417360305786,\n",
       " 0.2502346932888031,\n",
       " 0.38574090600013733,\n",
       " 0.12100965529680252,\n",
       " 0.3146654963493347,\n",
       " 0.1774149388074875,\n",
       " 0.26487967371940613,\n",
       " 0.28602081537246704,\n",
       " 0.35708168148994446,\n",
       " 0.13841046392917633,\n",
       " 0.08720145374536514,\n",
       " 0.2618696689605713,\n",
       " 0.3292483687400818,\n",
       " 0.2004483938217163,\n",
       " 0.5093190670013428,\n",
       " 0.3132331073284149,\n",
       " 0.12795133888721466,\n",
       " 0.2626314163208008,\n",
       " 0.17158709466457367,\n",
       " 0.13985411822795868,\n",
       " 0.12734633684158325,\n",
       " 0.179825559258461,\n",
       " 0.5659019351005554,\n",
       " 0.2716294229030609,\n",
       " 0.23601830005645752,\n",
       " 0.12040432542562485,\n",
       " 0.29916617274284363,\n",
       " 0.11130634695291519,\n",
       " 0.16378377377986908,\n",
       " 0.29615315794944763,\n",
       " 0.3213765025138855,\n",
       " 0.2861168086528778,\n",
       " 0.26031628251075745,\n",
       " 0.2561126947402954,\n",
       " 0.19376975297927856,\n",
       " 0.1605895459651947,\n",
       " 0.2535902261734009,\n",
       " 0.24004420638084412,\n",
       " 0.2476220279932022,\n",
       " 0.1913432478904724,\n",
       " 0.1606268286705017,\n",
       " 0.11805399507284164,\n",
       " 0.32297423481941223,\n",
       " 0.26884979009628296,\n",
       " 0.30374008417129517,\n",
       " 0.3874046504497528,\n",
       " 0.400300532579422,\n",
       " 0.2712182104587555,\n",
       " 0.32897254824638367,\n",
       " 0.34268471598625183,\n",
       " 0.157491073012352,\n",
       " 0.3416297435760498,\n",
       " 0.18034900724887848,\n",
       " 0.1679200530052185,\n",
       " 0.19852446019649506,\n",
       " 0.14102569222450256,\n",
       " 0.2092469483613968,\n",
       " 0.19846686720848083,\n",
       " 0.29035040736198425,\n",
       " 0.21680139005184174,\n",
       " 0.17691907286643982,\n",
       " 0.3024824261665344,\n",
       " 0.22856654226779938,\n",
       " 0.13097824156284332,\n",
       " 0.22706319391727448,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_preds.reshape(len(train_final_preds)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78822b-3779-4cd1-a6f7-93f975461005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parm_grid_bytes = pickle.dumps(param_grid_history)\n",
    "with open(f\"rendered_data/{objective}_bytes.hex\", \"wb\") as binary_file:\n",
    "    binary_file.write(parm_grid_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1f5eb-5007-4b73-a696-7c1b938a12cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(param_grid):\n",
    "    reg = xgb.XGBModel(\n",
    "        # These parameters should help with trial speed.\n",
    "        objective='binary:logistic',\n",
    "        tree_method='gpu_hist',\n",
    "        booster='gbtree',\n",
    "        predictor='gpu_predictor',\n",
    "        n_jobs=4,\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=100,\n",
    "        **param_grid\n",
    "    )\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.20)\n",
    "    \n",
    "    reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "        \n",
    "    return reg\n",
    "\n",
    "percent = math.ceil(len(param_grid_history.keys()) * .1)\n",
    "top = sorted(list(param_grid_history.keys()))[-percent:]\n",
    "\n",
    "train_preds = []\n",
    "test_preds = []\n",
    "for key in tqdm(top):\n",
    "    model = train(param_grid_history[key])\n",
    "    train_preds.append(model.predict(train_df[features]))\n",
    "    test_preds.append(model.predict(test_df[features]))\n",
    "\n",
    "train_final_preds = np.stack(train_preds).mean(0)\n",
    "test_final_preds = np.stack(test_preds).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a849ba6-0559-42be-88da-2b5f6738d4f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_final_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: train_df\u001b[38;5;241m.\u001b[39mindex, target[\u001b[38;5;241m0\u001b[39m]: \u001b[43mtrain_final_preds\u001b[49m})\n\u001b[0;32m      2\u001b[0m train_submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrendered_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobjective\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m test_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: test_df\u001b[38;5;241m.\u001b[39mindex, target[\u001b[38;5;241m0\u001b[39m]: test_final_preds})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_final_preds' is not defined"
     ]
    }
   ],
   "source": [
    "train_submission = pd.DataFrame(data={'id': train_df.index, target[0]: train_final_preds})\n",
    "train_submission.to_csv(fr'rendered_data/{objective}_train_submission.csv', index=False)\n",
    "\n",
    "test_submission = pd.DataFrame(data={'id': test_df.index, target[0]: test_final_preds})\n",
    "test_submission.to_csv(fr'rendered_data/{objective}_test_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39940d7-295b-4781-b008-0faa28ecb553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4101df8-7385-492d-b06e-7f42d874cf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff80b8-37d3-44e2-9082-25cc759d93a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e7362-c2ca-4bee-8666-9c4e305e1fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872ad54-57a6-4511-9389-3a46ad31366f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
