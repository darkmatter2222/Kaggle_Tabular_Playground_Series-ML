{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 2.138608,
     "end_time": "2021-02-19T02:57:47.964373",
     "exception": false,
     "start_time": "2021-02-19T02:57:45.825765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1=[[1,1,1],[3,3,3]]\n",
    "t2=[[1,1,1],[3,3,3]]\n",
    "test_t=[]\n",
    "test_t[len(test_t):] = t1\n",
    "test_t[len(test_t):] = t2\n",
    "np.save(f'test.npy', test_t)\n",
    "test_l = np.load('test.npy')\n",
    "np.array(test_l).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 4.206009,
     "end_time": "2021-02-19T02:57:52.183073",
     "exception": false,
     "start_time": "2021-02-19T02:57:47.977064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('..\\\\kaggle_data\\\\train.csv')\n",
    "test = pd.read_csv('..\\\\kaggle_data\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.095898,
     "end_time": "2021-02-19T02:57:52.291780",
     "exception": false,
     "start_time": "2021-02-19T02:57:52.195882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "y_train = train.target\n",
    "X_test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.020649,
     "end_time": "2021-02-19T02:57:52.324883",
     "exception": false,
     "start_time": "2021-02-19T02:57:52.304234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = [feature for feature in train.columns if 'cat' in feature]\n",
    "\n",
    "def label_encoder(df):\n",
    "    for feature in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df[feature])\n",
    "        df[feature] = le.transform(df[feature])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 1.818185,
     "end_time": "2021-02-19T02:57:54.155645",
     "exception": false,
     "start_time": "2021-02-19T02:57:52.337460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = label_encoder(X_train)\n",
    "X_test = label_encoder(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold index 2\n",
      "RMSE for Base model is 0.844066621029272\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=17.335285595031994 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=10.987474846877767 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 1 model is 0.8440215983638926\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=17.335285595031994 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=10.987474846877767 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 2 model is 0.8439865432619419\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=15.601757035528795 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=9.888727362189991 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 3 model is 0.8438800134974772\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=14.041581331975916 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=8.899854625970992 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 4 model is 0.8438307091606543\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=12.637423198778325 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=8.009869163373892 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 5 model is 0.8438080794612002\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=11.373680878900494 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=7.208882247036503 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 6 model is 0.84379593077556\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=10.236312791010445 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=6.487994022332853 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 7 model is 0.8437896456875694\n",
      "\n",
      "\n",
      "Improvement of : 0.00027697534170256777\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:26, 86.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=17.335285595031994 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=10.987474846877767 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Base model is 0.8444091042439591\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=17.335285595031994 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=10.987474846877767 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 1 model is 0.8443485924213023\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=17.335285595031994 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=10.987474846877767 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 2 model is 0.8443047074734082\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=15.601757035528795 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=9.888727362189991 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 3 model is 0.8442014004658082\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=14.041581331975916 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=8.899854625970992 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 4 model is 0.8441427147899908\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=12.637423198778325 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=8.009869163373892 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 5 model is 0.8441391336537261\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=11.373680878900494 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=7.208882247036503 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 6 model is 0.8441295350891671\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] verbosity is set=-1, verbose=-1 will be ignored. Current value: verbosity=-1\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.4016532152438, reg_alpha=10.236312791010445 will be ignored. Current value: lambda_l1=9.4016532152438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7091814665169984, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7091814665169984\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.006560267182579874, reg_lambda=6.487994022332853 will be ignored. Current value: lambda_l2=0.006560267182579874\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "RMSE for Incremental trial 7 model is 0.844127999166191\n",
      "\n",
      "\n",
      "Improvement of : 0.000281105077768129\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [03:02, 182.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8e43902c4d99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Improvement of : {first_rmse - last_rmse}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mpreds_list_final_iteration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mpreds_list_final_iterations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds_list_final_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_list_final_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mdt_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d-%m-%Y_%H-%M-%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ryans\\source\\repos\\venvs\\python375\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m                              % (self._n_features, n_features))\n\u001b[0;32m    687\u001b[0m         return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n\u001b[1;32m--> 688\u001b[1;33m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ryans\\source\\repos\\venvs\\python375\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[0;32m   2957\u001b[0m         return predictor.predict(data, start_iteration, num_iteration,\n\u001b[0;32m   2958\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2959\u001b[1;33m                                  data_has_header, is_reshape)\n\u001b[0m\u001b[0;32m   2960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2961\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ryans\\source\\repos\\venvs\\python375\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__pred_for_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m             \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__pred_for_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ryans\\source\\repos\\venvs\\python375\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[1;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0minner_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     def __create_sparse_native(self, cs, out_shape, out_ptr_indptr, out_ptr_indices, out_ptr_data,\n",
      "\u001b[1;32mc:\\users\\ryans\\source\\repos\\venvs\\python375\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36minner_predict\u001b[1;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[0;32m    657\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_parameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_num_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[0;32m    660\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn_preds\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mout_num_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Wrong length for predict results\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fold_ittr_range = 10\n",
    "preds_list_final_iterations =[]\n",
    "\n",
    "for fold_index in range(2, fold_ittr_range):\n",
    "    print(f'Current fold index {fold_index}')\n",
    "    split = KFold(n_splits=fold_index)\n",
    "    lgbm_params = {'metric': 'rmse', \n",
    "                   'verbosity': -1, \n",
    "                   'boosting_type': 'gbdt', \n",
    "                   'feature_pre_filter': False, \n",
    "                   'lambda_l1': 9.4016532152438, \n",
    "                   'lambda_l2': 0.006560267182579874, \n",
    "                   'num_leaves': 10, \n",
    "                   'feature_fraction': 0.4, \n",
    "                   'bagging_fraction': 0.7091814665169984, \n",
    "                   'bagging_freq': 7, \n",
    "                   'min_child_samples': 25, \n",
    "                   'num_iterations': 1000, \n",
    "                   'early_stopping_round': 100,\n",
    "                    'reg_lambda': 10.987474846877767, \n",
    "                'reg_alpha': 17.335285595031994,\n",
    "                  'verbose':-1}\n",
    "    \n",
    "\n",
    "    preds_list_base = []\n",
    "    preds_list_final_iteration = []\n",
    "    preds_list_all = []\n",
    "\n",
    "    for train_idx, val_idx in tqdm(split.split(X_train)):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                      eval_metric=['rmse'],\n",
    "                      early_stopping_rounds=250, \n",
    "                      categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                      #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                      verbose=0)\n",
    "\n",
    "        preds_list_base.append(Model.predict(X_test))\n",
    "        preds_list_all.append(Model.predict(X_test))\n",
    "        print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "        first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "        params = lgbm_params.copy()\n",
    "\n",
    "        for i in range(1, 8):\n",
    "            if i >2:    \n",
    "\n",
    "                # reducing regularizing params if \n",
    "\n",
    "                params['reg_lambda'] *= 0.9\n",
    "                params['reg_alpha'] *= 0.9\n",
    "                params['num_leaves'] += 40\n",
    "\n",
    "            params['learning_rate'] = 0.003\n",
    "            Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                      eval_metric=['rmse'],\n",
    "                      early_stopping_rounds=200, \n",
    "                      categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                      #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                      verbose=0,\n",
    "                      init_model=Model)\n",
    "\n",
    "            preds_list_all.append(Model.predict(X_test))\n",
    "            print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "        last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "        print('',end='\\n\\n')\n",
    "        print(f'Improvement of : {first_rmse - last_rmse}')\n",
    "        print('-' * 100)\n",
    "        preds_list_final_iteration.append(Model.predict(X_test))\n",
    "    preds_list_final_iterations[len(preds_list_final_iterations):] = preds_list_final_iteration\n",
    "    dt_string = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    np.save(f'..\\\\kaggle_data\\\\{dt_string}_FI{fold_index}_sub.npy', preds_list_final_iterations)\n",
    "    y_preds_final_iteration = np.array(preds_list_final_iterations).mean(axis=0)\n",
    "    submission = pd.DataFrame({'id':test.id,\n",
    "              'target':y_preds_final_iteration})\n",
    "    submission.to_csv(f'..\\\\kaggle_data\\\\{dt_string}_FI{fold_index}_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_index in range(20, 30):\n",
    "    print(f'Current fold index {fold_index}')\n",
    "    split = KFold(n_splits=fold_index)\n",
    "    lgbm_params = {'max_depth': 16, \n",
    "                'subsample': 0.8032697250789377, \n",
    "                'colsample_bytree': 0.21067140508531404, \n",
    "                'learning_rate': 0.009867383057779643,\n",
    "                'reg_lambda': 10.987474846877767, \n",
    "                'reg_alpha': 17.335285595031994, \n",
    "                'min_child_samples': 31, \n",
    "                'num_leaves': 66, \n",
    "                'max_bin': 522, \n",
    "                'cat_smooth': 81, \n",
    "                'cat_l2': 0.029690334194270022, \n",
    "                'metric': 'rmse', \n",
    "                'n_jobs': -1, \n",
    "                'n_estimators': 20000,\n",
    "                'boosting_type': 'gbdt'\n",
    "                  }\n",
    "    preds_list_base = []\n",
    "    preds_list_final_iteration = []\n",
    "    preds_list_all = []\n",
    "\n",
    "    for train_idx, val_idx in tqdm(split.split(X_train)):\n",
    "                X_tr = X_train.iloc[train_idx]\n",
    "                X_val = X_train.iloc[val_idx]\n",
    "                y_tr = y_train.iloc[train_idx]\n",
    "                y_val = y_train.iloc[val_idx]\n",
    "\n",
    "                Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                              eval_metric=['rmse'],\n",
    "                              early_stopping_rounds=250, \n",
    "                              categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                              verbose=0)\n",
    "\n",
    "                preds_list_base.append(Model.predict(X_test))\n",
    "                preds_list_all.append(Model.predict(X_test))\n",
    "                print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "                first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "                params = lgbm_params.copy()\n",
    "\n",
    "                for i in range(1, 8):\n",
    "                    if i >2:    \n",
    "\n",
    "                        # reducing regularizing params if \n",
    "\n",
    "                        params['reg_lambda'] *= 0.9\n",
    "                        params['reg_alpha'] *= 0.9\n",
    "                        params['num_leaves'] += 40\n",
    "\n",
    "                    params['learning_rate'] = 0.003\n",
    "                    Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                              eval_metric=['rmse'],\n",
    "                              early_stopping_rounds=200, \n",
    "                              categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                              verbose=0,\n",
    "                              init_model=Model)\n",
    "\n",
    "                    preds_list_all.append(Model.predict(X_test))\n",
    "                    print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "                last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "                print('',end='\\n\\n')\n",
    "                print(f'Improvement of : {first_rmse - last_rmse}')\n",
    "                print('-' * 100)\n",
    "                preds_list_final_iteration.append(Model.predict(X_test))\n",
    "    preds_list_final_iterations[len(preds_list_final_iterations):] = preds_list_final_iteration\n",
    "    dt_string = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    np.save(f'..\\\\kaggle_data\\\\{dt_string}_FI{fold_index}_sub.npy', preds_list_final_iterations)\n",
    "    y_preds_final_iteration = np.array(preds_list_final_iterations).mean(axis=0)\n",
    "    submission = pd.DataFrame({'id':test.id,\n",
    "              'target':y_preds_final_iteration})\n",
    "    submission.to_csv(f'..\\\\kaggle_data\\\\{dt_string}_FI{fold_index}_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_ittr_range = 10\n",
    "preds_list_final_iterations =[]\n",
    "\n",
    "fold_index = 30\n",
    "print(f'Current fold index {fold_index}')\n",
    "split = KFold(n_splits=fold_index)\n",
    "lgbm_params = {'max_depth': 16, \n",
    "            'subsample': 0.8032697250789377, \n",
    "            'colsample_bytree': 0.21067140508531404, \n",
    "            'learning_rate': 0.009867383057779643,\n",
    "            'reg_lambda': 10.987474846877767, \n",
    "            'reg_alpha': 17.335285595031994, \n",
    "            'min_child_samples': 31, \n",
    "            'num_leaves': 66, \n",
    "            'max_bin': 522, \n",
    "            'cat_smooth': 81, \n",
    "            'cat_l2': 0.029690334194270022, \n",
    "            'metric': 'rmse', \n",
    "            'n_jobs': -1, \n",
    "            'n_estimators': 20000}\n",
    "\n",
    "\n",
    "preds_list_base = []\n",
    "preds_list_final_iteration = []\n",
    "preds_list_all = []\n",
    "\n",
    "for train_idx, val_idx in tqdm(split.split(X_train)):\n",
    "    X_tr = X_train.iloc[train_idx]\n",
    "    X_val = X_train.iloc[val_idx]\n",
    "    y_tr = y_train.iloc[train_idx]\n",
    "    y_val = y_train.iloc[val_idx]\n",
    "\n",
    "    Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                  eval_metric=['rmse'],\n",
    "                  early_stopping_rounds=250, \n",
    "                  categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                  #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                  verbose=0)\n",
    "\n",
    "    preds_list_base.append(Model.predict(X_test))\n",
    "    preds_list_all.append(Model.predict(X_test))\n",
    "    print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "    first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "    params = lgbm_params.copy()\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        if i >2:    \n",
    "\n",
    "            # reducing regularizing params if \n",
    "\n",
    "            params['reg_lambda'] *= 0.9\n",
    "            params['reg_alpha'] *= 0.9\n",
    "            params['num_leaves'] += 40\n",
    "\n",
    "        params['learning_rate'] = 0.003\n",
    "        Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                  eval_metric=['rmse'],\n",
    "                  early_stopping_rounds=200, \n",
    "                  categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                  #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                  verbose=0,\n",
    "                  init_model=Model)\n",
    "\n",
    "        preds_list_all.append(Model.predict(X_test))\n",
    "        print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "    last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "    print('',end='\\n\\n')\n",
    "    print(f'Improvement of : {first_rmse - last_rmse}')\n",
    "    print('-' * 100)\n",
    "    preds_list_final_iteration.append(Model.predict(X_test))\n",
    "    \n",
    "preds_list_final_iterations[len(preds_list_final_iterations):] = preds_list_final_iteration\n",
    "dt_string = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "np.save(f'..\\\\kaggle_data\\\\{dt_string}_FI{fold_index}_sub.npy', preds_list_final_iterations)\n",
    "y_preds_final_iteration = np.array(preds_list_final_iterations).mean(axis=0)\n",
    "submission = pd.DataFrame({'id':test.id,\n",
    "          'target':y_preds_final_iteration})\n",
    "submission.to_csv(f'..\\\\kaggle_data\\\\{dt_string}_FI{fold_index}_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds_list_final_iterations[-165:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_string = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "y_preds_final_iteration = np.array(preds_list_final_iterations[-20:]).mean(axis=0)\n",
    "submission = pd.DataFrame({'id':test.id,\n",
    "          'target':y_preds_final_iteration})\n",
    "submission.to_csv(f'..\\\\kaggle_data\\\\{dt_string}_FI00_sub.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('10_ittr_res.npy', 'wb') as f:\n",
    "    np.save(f, np.array([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_final_iteration = np.array(preds_list_final_iterations).mean(axis=0)\n",
    "y_preds_final_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':test.id,\n",
    "              'target':y_preds_final_iteration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dt_string = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "submission.to_csv(f'..\\\\kaggle_data\\\\{dt_string}_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.020441,
     "end_time": "2021-02-19T02:57:54.188643",
     "exception": false,
     "start_time": "2021-02-19T02:57:54.168202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021692,
     "end_time": "2021-02-19T02:57:54.223133",
     "exception": false,
     "start_time": "2021-02-19T02:57:54.201441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_params = {'max_depth': 16, \n",
    "                'subsample': 0.8032697250789377, \n",
    "                'colsample_bytree': 0.21067140508531404, \n",
    "                'learning_rate': 0.009867383057779643,\n",
    "                'reg_lambda': 10.987474846877767, \n",
    "                'reg_alpha': 17.335285595031994, \n",
    "                'min_child_samples': 31, \n",
    "                'num_leaves': 66, \n",
    "                'max_bin': 522, \n",
    "                'cat_smooth': 81, \n",
    "                'cat_l2': 0.029690334194270022, \n",
    "                'metric': 'rmse', \n",
    "                'n_jobs': -1, \n",
    "                'n_estimators': 20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5357.717902,
     "end_time": "2021-02-19T04:27:11.954129",
     "exception": false,
     "start_time": "2021-02-19T02:57:54.236227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_list_base = []\n",
    "preds_list_final_iteration = []\n",
    "preds_list_all = []\n",
    "\n",
    "for train_idx, val_idx in split.split(X_train):\n",
    "            X_tr = X_train.iloc[train_idx]\n",
    "            X_val = X_train.iloc[val_idx]\n",
    "            y_tr = y_train.iloc[train_idx]\n",
    "            y_val = y_train.iloc[val_idx]\n",
    "            \n",
    "            Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                          eval_metric=['rmse'],\n",
    "                          early_stopping_rounds=250, \n",
    "                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                          verbose=0)\n",
    "            \n",
    "            preds_list_base.append(Model.predict(X_test))\n",
    "            preds_list_all.append(Model.predict(X_test))\n",
    "            print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "            first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "            params = lgbm_params.copy()\n",
    "            \n",
    "            for i in range(1, 8):\n",
    "                if i >2:    \n",
    "                    \n",
    "                    # reducing regularizing params if \n",
    "                    \n",
    "                    params['reg_lambda'] *= 0.9\n",
    "                    params['reg_alpha'] *= 0.9\n",
    "                    params['num_leaves'] += 40\n",
    "                    \n",
    "                params['learning_rate'] = 0.003\n",
    "                Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                          eval_metric=['rmse'],\n",
    "                          early_stopping_rounds=200, \n",
    "                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                          verbose=0,\n",
    "                          init_model=Model)\n",
    "                \n",
    "                preds_list_all.append(Model.predict(X_test))\n",
    "                print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "            last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "            print('',end='\\n\\n')\n",
    "            print(f'Improvement of : {first_rmse - last_rmse}')\n",
    "            print('-' * 100)\n",
    "            preds_list_final_iteration.append(Model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in split.split(X_train):\n",
    "            X_tr = X_train.iloc[train_idx]\n",
    "            X_val = X_train.iloc[val_idx]\n",
    "            y_tr = y_train.iloc[train_idx]\n",
    "            y_val = y_train.iloc[val_idx]\n",
    "            \n",
    "            Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                          eval_metric=['rmse'],\n",
    "                          early_stopping_rounds=250, \n",
    "                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                          verbose=0)\n",
    "            \n",
    "            preds_list_base.append(Model.predict(X_test))\n",
    "            preds_list_all.append(Model.predict(X_test))\n",
    "            print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "            first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "            params = lgbm_params.copy()\n",
    "            \n",
    "            for i in range(1, 8):\n",
    "                if i >2:    \n",
    "                    \n",
    "                    # reducing regularizing params if \n",
    "                    \n",
    "                    params['reg_lambda'] *= 0.9\n",
    "                    params['reg_alpha'] *= 0.9\n",
    "                    params['num_leaves'] += 40\n",
    "                    \n",
    "                params['learning_rate'] = 0.003\n",
    "                Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                          eval_metric=['rmse'],\n",
    "                          early_stopping_rounds=200, \n",
    "                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                          verbose=0,\n",
    "                          init_model=Model)\n",
    "                \n",
    "                preds_list_all.append(Model.predict(X_test))\n",
    "                print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "            last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "            print('',end='\\n\\n')\n",
    "            print(f'Improvement of : {first_rmse - last_rmse}')\n",
    "            print('-' * 100)\n",
    "            preds_list_final_iteration.append(Model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.04053,
     "end_time": "2021-02-19T04:27:12.129613",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.089083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_preds_base = np.array(preds_list_base).mean(axis=0)\n",
    "y_preds_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.072683,
     "end_time": "2021-02-19T04:27:12.232996",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.160313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_preds_all = np.array(preds_list_all).mean(axis=0)\n",
    "y_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040136,
     "end_time": "2021-02-19T04:27:12.301392",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.261256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_preds_final_iteration = np.array(preds_list_final_iteration).mean(axis=0)\n",
    "y_preds_final_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.037132,
     "end_time": "2021-02-19T04:27:12.366658",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.329526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':test.id,\n",
    "              'target':y_preds_final_iteration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.75707,
     "end_time": "2021-02-19T04:27:13.151592",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.394522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.028568,
     "end_time": "2021-02-19T04:27:13.410908",
     "exception": false,
     "start_time": "2021-02-19T04:27:13.382340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5375.500553,
   "end_time": "2021-02-19T04:27:15.130226",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-19T02:57:39.629673",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
